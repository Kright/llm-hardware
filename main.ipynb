{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "\n",
    "task_ids = set([2,255,4,135,8,137,138,136,7,269,397,15,17,18,276,21,152,286,159,160,161,164,46,181,196,203,209,84,85,217,352,230,105,235,239,246,249,378,251,127])\n",
    "ollama_models = [\"llama2-uncensored:7b\", \"deepseek-r1:7b\", \"qwen2.5:14b\", \"phi4:14b\", \"deepseek-r1:32b\", \"qwen2.5:32b\", \"command-r:latest\", \"dolphin-mixtral:8x7b\", \"deepseek-r1:70b\", \"llama3.3:70b\"]\n",
    "oai_models = [\"gpt-4o\", \"o1-mini\", \"o1-preview\"]\n",
    "models = set(ollama_models + oai_models)\n",
    "\n",
    "with open(\"in-prompts.json\", \"r\") as file:\n",
    "    prompts = json.load(file)\n",
    "\n",
    "runs = defaultdict(dict)\n",
    "\n",
    "for file in os.listdir(\".\"):\n",
    "    if not file.startswith(\"out-results\"):\n",
    "        continue\n",
    "\n",
    "    with open(file, \"r\") as file:\n",
    "        for line in file:\n",
    "            item = json.loads(line)\n",
    "            model = item.get(\"model\")\n",
    "            runner = \"oai\" if (model in [\"gpt-4o\", \"o1-mini\"]) else item.get(\"runner\", \"unknown\")\n",
    "            task_id = item.get(\"task_id\")\n",
    "\n",
    "            if task_id in task_ids and model in models and \"result\" in item and \"elapsed_time\" in item:\n",
    "                runs[(runner, model)][task_id] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "stats = defaultdict(lambda:defaultdict(lambda:\"N/A\"))\n",
    "\n",
    "def get_text(result):\n",
    "    try:\n",
    "        return result[\"result\"][\"message\"][\"content\"]\n",
    "    except Exception:\n",
    "        try:\n",
    "            return result[\"result\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "for (runner, model), value in runs.items():\n",
    "    stats[runner][model] = len(value)\n",
    "    # if len(value) == len(task_ids):\n",
    "        # perfs = [len(prompts[task_id] + get_text(result)) / result.get(\"elapsed_time\") for task_id, result in value.items()]\n",
    "        # stats[runner][model] = f\"{int(np.percentile(perfs, 10))}-{int(np.percentile(perfs, 90))}\"\n",
    "        # stats[runner][model] = int(np.percentile(perfs, 50))\n",
    "\n",
    "print(json.dumps(stats, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame({runner: [data[model] for model in ollama_models] for runner, data in stats.items()})"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
