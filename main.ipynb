{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "prompts, names, runs = {}, {}, []\n",
    "\n",
    "with open(\"main.json.log\", \"r\") as file:\n",
    "    for line in file:\n",
    "        item = json.loads(line)\n",
    "\n",
    "        if item[\"type\"] == \"name\":\n",
    "            names[item[\"key\"]] = item[\"name\"]\n",
    "        elif item[\"type\"] == \"prompt\" and item.get(\"main\"):\n",
    "            prompts[item[\"prompt_id\"]] = item[\"prompt\"]\n",
    "        elif item[\"type\"] == \"run\" and \"result\" in item:\n",
    "            runs.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def get_text(result):\n",
    "    try:\n",
    "        return result[\"result\"][\"message\"][\"content\"]\n",
    "    except Exception:\n",
    "        try:\n",
    "            return result[\"result\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "with open(\"out-results.json.log\", \"r\") as file:\n",
    "    results = []\n",
    "\n",
    "    for line in file:\n",
    "        result = json.loads(line)\n",
    "        model = result.get(\"model\")\n",
    "        runner = \"oai\" if (model in oai_models) else result.get(\"runner\")\n",
    "        prompt_id = result.get(\"task_id\")\n",
    "\n",
    "        if \"result\" in result and prompt_id in prompts_corpus and \"elapsed_time\" in result and runner in runners and model in models:\n",
    "            results.append({\n",
    "                \"runner\": runners[runner],\n",
    "                \"model\": models[model],\n",
    "                \"_runner\": runner,\n",
    "                \"_model\": model,\n",
    "                \"prompt_id\": prompt_id,\n",
    "                \"prompt\": prompts[prompt_id],\n",
    "                \"answer\": get_text(result),\n",
    "                \"total_len\": len(prompts[prompt_id]) + len(get_text(result)),\n",
    "                \"elapsed_time\": result[\"elapsed_time\"]\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count = df.groupby([\"runner\", \"model\"]).size().reset_index(name=\"num_items\")\n",
    "df_count.pivot(index=\"model\", columns=\"runner\", values=\"num_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df.groupby([\"runner\", \"model\"]).filter(lambda g: len(g) == len(prompts_corpus))\n",
    "df_filtered[\"ratio\"] = df_filtered[\"total_len\"] / df_filtered[\"elapsed_time\"]\n",
    "\n",
    "df_median = df_filtered.groupby([\"runner\", \"model\"])[\"ratio\"].median().reset_index()\n",
    "table = df_median.pivot(index=\"model\", columns=\"runner\", values=\"ratio\")\n",
    "\n",
    "table.index.name = None\n",
    "table.columns.name = None\n",
    "\n",
    "runners_sorted = [r for r in runners.values() if r in table.columns]\n",
    "models_sorted = [r for r in models.values() if r in table.index]\n",
    "\n",
    "table = table.loc[models_sorted, runners_sorted]\n",
    "table.applymap(lambda x: int(x) if pd.notna(x) else \"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
